{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i think this combines everything that i did below so i'll remove/clean up the extra stuff above once i confirm this function works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(samples, dim, optimizer_list, criterion, test_runs):\n",
    "    loss_list = []\n",
    "    \n",
    "    samples = samples #number of samples from each distribution\n",
    "    n = dim\n",
    "\n",
    "    for i in range(test_runs):\n",
    "        \n",
    "        test_run_loss = []\n",
    "        \n",
    "        # get random samples from normal(0,1) distribution\n",
    "        x_dataset = torch.randn((samples, n))\n",
    "        linear_factor = torch.randn(n, 1) # We need to use the same linear factor for the test data\n",
    "        y_dataset = torch.matmul(x_dataset, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise\n",
    "\n",
    "        # get testing samples\n",
    "        x_test = torch.randn((samples, n))\n",
    "        y_test = torch.matmul(x_test, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise. same linear factor as training data\n",
    "        \n",
    "        for optimizer in optimizer_list:\n",
    "            \n",
    "            # Linear regression model\n",
    "            model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n, 1)\n",
    "            )\n",
    "            \n",
    "            train(optimizer, x_dataset, y_dataset, model, criterion)\n",
    "            test_run_loss.append(test(model, x_test, y_test, criterion))\n",
    "            \n",
    "        loss_list.append(test_run_loss)\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the average loss for each optimizer over several test runs.\n",
    "\"\"\"\n",
    "def calc_task_avg_loss(loss_list):\n",
    "    avg_loss = len(loss_list[0])*[0]\n",
    "    for test_run in range(len(loss_list)):\n",
    "        for optimizer in range(len(loss_list[test_run])):\n",
    "            avg_loss[optimizer] += loss_list[test_run][optimizer]\n",
    "\n",
    "    for i in range(len(avg_loss)):\n",
    "        avg_loss[i] /= len(loss_list) \n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of optimizers to loop through\n",
    "optimizer_list=[]\n",
    "optimizer_list.append(optim.SGD(model.parameters(), lr=0.01))\n",
    "optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9))\n",
    "optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True))\n",
    "optimizer_list.append(optim.Adagrad(model.parameters(), lr=0.01))\n",
    "optimizer_list.append(optim.RMSprop(model.parameters(), lr=0.01))\n",
    "optimizer_list.append(optim.Adam(model.parameters(), lr=0.01))\n",
    "\n",
    "# Loss\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = [] # store average losses for each type of parameterization\n",
    "test_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 103.57756042480469\n",
      "Loss: 96.18888854980469\n",
      "loss = 106.38829040527344\n",
      "Loss: 98.74159240722656\n",
      "loss = 107.45361328125\n",
      "Loss: 98.94036865234375\n",
      "loss = 105.07817077636719\n",
      "Loss: 94.28800964355469\n",
      "loss = 108.47374725341797\n",
      "Loss: 99.03488159179688\n",
      "loss = 104.93363189697266\n",
      "Loss: 96.7685775756836\n",
      "loss = 126.70720672607422\n",
      "Loss: 98.69624328613281\n",
      "loss = 129.86695861816406\n",
      "Loss: 100.77242279052734\n",
      "loss = 123.75627899169922\n",
      "Loss: 100.0716323852539\n",
      "loss = 123.70311737060547\n",
      "Loss: 99.52873229980469\n",
      "loss = 126.77615356445312\n",
      "Loss: 105.5424575805664\n",
      "loss = 127.20201873779297\n",
      "Loss: 105.12014770507812\n",
      "loss = 108.95611572265625\n",
      "Loss: 94.65900421142578\n",
      "loss = 106.96150207519531\n",
      "Loss: 92.64361572265625\n",
      "loss = 105.92987060546875\n",
      "Loss: 95.18988037109375\n",
      "loss = 105.5101547241211\n",
      "Loss: 92.11715698242188\n",
      "loss = 108.13522338867188\n",
      "Loss: 96.44393920898438\n",
      "loss = 106.72901153564453\n",
      "Loss: 93.26188659667969\n",
      "loss = 108.00425720214844\n",
      "Loss: 88.32024383544922\n",
      "loss = 104.83251953125\n",
      "Loss: 88.68097686767578\n",
      "loss = 107.40242004394531\n",
      "Loss: 90.70246124267578\n",
      "loss = 106.46118927001953\n",
      "Loss: 88.99043273925781\n",
      "loss = 107.16880798339844\n",
      "Loss: 88.62568664550781\n",
      "loss = 102.61058807373047\n",
      "Loss: 88.32599639892578\n",
      "loss = 100.97868347167969\n",
      "Loss: 104.89688110351562\n",
      "loss = 103.43502807617188\n",
      "Loss: 107.09361267089844\n",
      "loss = 106.50296020507812\n",
      "Loss: 107.4444351196289\n",
      "loss = 103.46591186523438\n",
      "Loss: 103.09820556640625\n",
      "loss = 102.01457214355469\n",
      "Loss: 103.95706939697266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-0bb3d7b36f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mexact_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_task_avg_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexact_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-272-4a46f0d6951b>\u001b[0m in \u001b[0;36mget_losses\u001b[0;34m(samples, dim, optimizer_list, criterion, test_runs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtest_run_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-19a8821d5f2b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(optimizer, x_dataset, y_dataset, model, loss)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Update A and b accordingly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss = {current_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     print(f\"t = {t}, loss = {current_loss}, A = {A.detach().numpy()}, b = {b.item()}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# exactly parameterized\n",
    "samples = 100 #number of samples from each distribution\n",
    "dim = 100\n",
    "\n",
    "exact_param = get_losses(samples, dim, optimizer_list, criterion, test_runs)\n",
    "all_losses.append(calc_task_avg_loss(exact_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overparameterized\n",
    "samples = 100 #number of samples from each distribution\n",
    "dim = 200\n",
    "\n",
    "over_param = get_losses(samples, dim, optimizer_list, criterion, test_runs)\n",
    "all_losses.append(calc_task_avg_loss(over_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# underparameterized\n",
    "samples = 100 #number of samples from each distribution\n",
    "dim = 3\n",
    "\n",
    "under_param = get_losses(samples, dim, optimizer_list, criterion, test_runs)\n",
    "all_losses.append(calc_task_avg_loss(under_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SGD</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>Nesterov</th>\n",
       "      <th>Adagrad</th>\n",
       "      <th>RMSProp</th>\n",
       "      <th>Adam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic_regression_gaussian_exact_param</th>\n",
       "      <td>4.170680</td>\n",
       "      <td>4.158137</td>\n",
       "      <td>4.142331</td>\n",
       "      <td>4.137851</td>\n",
       "      <td>4.140847</td>\n",
       "      <td>4.156715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_gaussian_over_param</th>\n",
       "      <td>4.060101</td>\n",
       "      <td>4.064485</td>\n",
       "      <td>4.097406</td>\n",
       "      <td>4.081241</td>\n",
       "      <td>4.067134</td>\n",
       "      <td>4.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_gaussian_over_param</th>\n",
       "      <td>4.501864</td>\n",
       "      <td>4.477825</td>\n",
       "      <td>4.482738</td>\n",
       "      <td>4.490011</td>\n",
       "      <td>4.513530</td>\n",
       "      <td>4.515857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               SGD  Momentum  Nesterov  \\\n",
       "logistic_regression_gaussian_exact_param  4.170680  4.158137  4.142331   \n",
       "logistic_regression_gaussian_over_param   4.060101  4.064485  4.097406   \n",
       "logistic_regression_gaussian_over_param   4.501864  4.477825  4.482738   \n",
       "\n",
       "                                           Adagrad   RMSProp      Adam  \n",
       "logistic_regression_gaussian_exact_param  4.137851  4.140847  4.156715  \n",
       "logistic_regression_gaussian_over_param   4.081241  4.067134  4.075100  \n",
       "logistic_regression_gaussian_over_param   4.490011  4.513530  4.515857  "
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = ['linear_regression_gaussian_exact_param','linear_regression_gaussian_over_param','linear_regression_gaussian_over_param']\n",
    "col = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "df = pd.DataFrame(data=all_losses, index=index, columns=col)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repetitive stuff below, will delete later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(optimizer, x_dataset, y_dataset, model,loss):\n",
    "    for t in range(1000):\n",
    "        # Set the gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the current predicted y from x_dataset\n",
    "        y_predicted = model(x_dataset)\n",
    "\n",
    "        current_loss = loss(y_predicted, y_dataset)\n",
    "        \n",
    "        # Compute the gradient of the loss with respect to A and b\n",
    "        current_loss.backward()\n",
    "        \n",
    "        # Update A and b accordingly.\n",
    "        optimizer.step()\n",
    "    print(f\"loss = {current_loss}\")\n",
    "#     print(f\"t = {t}, loss = {current_loss}, A = {A.detach().numpy()}, b = {b.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x_test, y_test, loss_fn):\n",
    "    # Returns accuracy, loss.\n",
    "    \n",
    "    # Get predicted probability vectors from test data.\n",
    "    y_predicted = model(x_test)\n",
    "\n",
    "    loss = loss_fn(y_predicted, y_test)\n",
    "    \n",
    "    print('Loss: {}'.format(loss.item()))\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exactly parameterized\n",
    "Number of parameters = number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 100 # number of predictor variables\n",
    "samples = 100\n",
    "\n",
    "x_dataset = torch.randn((samples, n))\n",
    "linear_factor = torch.randn(n, 1) # We need to use the same linear factor for the test data\n",
    "y_dataset = torch.matmul(x_dataset, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.randn((samples, n))\n",
    "y_test = torch.matmul(x_test, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise. same linear factor as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_param_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "exact_param_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, loss)\n",
    "exact_param_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "exact_param_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "exact_param_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "exact_param_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "exact_param_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss.append(exact_param_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overparameterized\n",
    "Number of parameters >> number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30 # number of predictor variables\n",
    "samples = 20\n",
    "\n",
    "# get random samples from normal(0,1) distribution\n",
    "x_dataset = torch.randn((samples, n))\n",
    "linear_factor = torch.randn(n, 1) # We need to use the same linear factor for the test data\n",
    "y_dataset = torch.matmul(x_dataset, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.randn((samples, n))\n",
    "y_test = torch.matmul(x_test, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise. same linear factor as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overparam_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "overparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "overparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "overparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "overparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "overparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "overparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss.append(overparam_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underparameterized\n",
    "Number of parameters << number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 # number of predictor variables\n",
    "samples = 100\n",
    "\n",
    "# get random samples from normal(0,1) distribution\n",
    "x_dataset = torch.randn((samples, n))\n",
    "linear_factor = torch.randn(n, 1) # We need to use the same linear factor for the test data\n",
    "y_dataset = torch.matmul(x_dataset, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.randn((samples, n))\n",
    "y_test = torch.matmul(x_test, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise. same linear factor as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "underparam_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "underparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "underparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "underparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "underparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "underparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train(optimizer, x_dataset, y_dataset, model,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test,y_test, loss)\n",
    "underparam_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss.append(underparam_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['Linear regression random samples - exactly parameterized','Linear regression random samples - overparameterized','Linear regression random samples - underparameterized']\n",
    "col = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "df = pd.DataFrame(data=all_loss, index=index, columns=col)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('linear_regression_gaussian_loss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss = np.asarray(all_loss)\n",
    "all_loss\n",
    "normalized_test_losses = []\n",
    "\n",
    "for i in range(len(all_loss)):\n",
    "    mean = np.mean(all_loss[i])\n",
    "    minus_mean = all_loss[i] - mean\n",
    "    normalized_test_losses.append((minus_mean)/np.linalg.norm(minus_mean))\n",
    "print(normalized_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['linear_regression_gaussian_exact_param','linear_regression_gaussian_over_param','linear_regression_gaussian_under_param']\n",
    "col = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "df = pd.DataFrame(data=normalized_test_losses, index=index, columns=col)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('linear_regression_gaussian_normalized_loss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overparameterized dimensions vs. losses graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_losses(samples, dim, optimizer, criterion):\n",
    "    samples = samples #number of samples from each distribution\n",
    "    n = dim\n",
    "\n",
    "    # get random samples from normal(0,1) distribution\n",
    "    x_dataset = torch.randn((samples, n))\n",
    "    linear_factor = torch.randn(n, 1) # We need to use the same linear factor for the test data\n",
    "    y_dataset = torch.matmul(x_dataset, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise\n",
    "    \n",
    "    # get testing samples\n",
    "    x_test = torch.randn((samples, n))\n",
    "    y_test = torch.matmul(x_test, linear_factor) + 0.1*torch.randn((samples, 1)) # Linear transform + random noise. same linear factor as training data\n",
    "    \n",
    "    # Linear regression model\n",
    "    model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n, 1)\n",
    "    )\n",
    "    \n",
    "    train(optimizer, x_dataset, y_dataset, model, criterion)\n",
    "    return test(model, x_test, y_test, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer_list=[]\n",
    "optimizer_list.append(optim.SGD(model.parameters(), lr=0.01))\n",
    "optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9))\n",
    "optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True))\n",
    "optimizer_list.append(optim.Adagrad(model.parameters(), lr=0.01))\n",
    "optimizer_list.append(optim.RMSprop(model.parameters(), lr=0.01))\n",
    "optimizer_list.append(optim.Adam(model.parameters(), lr=0.01))\n",
    "\n",
    "optimizer_names=['SGD', 'Momentum', 'Nesterov', 'Adagrad', 'RMSprop', 'Adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INDIVIDUAL GRAPHS\n",
    "\n",
    "samples = 100\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for opt in range(len(optimizer_list)):\n",
    "    losses = []\n",
    "    sample_sizes = []\n",
    "    for dim in range(1,500,20):\n",
    "        losses.append(get_losses(samples, dim, optimizer_list[opt], criterion))\n",
    "        sample_sizes.append(dim)\n",
    "    plt.plot(sample_sizes, losses)\n",
    "    plt.title('Linear regression losses for ' + optimizer_names[opt]+' with '+str(samples)+' samples')\n",
    "    plt.xlabel('Dimensions')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "#     plt.legend(['SGD', 'Momentum', 'Nesterov', 'Adagrad', 'RMSprop', 'Adam'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVERLAY GRAPH\n",
    "\n",
    "samples = 100\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for optimizer in optimizer_list:\n",
    "    losses = []\n",
    "    sample_sizes = []\n",
    "    for dim in range(1,500,20):\n",
    "        losses.append(get_losses(samples, dim, optimizer, criterion))\n",
    "        sample_sizes.append(dim)\n",
    "    plt.plot(sample_sizes, losses)\n",
    "    plt.title('Linear regression losses with '+str(samples)+' samples')\n",
    "    plt.xlabel('Dimensions')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['SGD', 'Momentum', 'Nesterov', 'Adagrad', 'RMSprop', 'Adam'], loc='upper left',bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
