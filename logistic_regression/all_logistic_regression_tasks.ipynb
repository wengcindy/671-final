{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 100\n",
    "test_runs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For a dataset, will loop through all the optimizers and save the test losses after training + testing.\n",
    "\"\"\"\n",
    "\n",
    "def train_test(train_loader, test_loader, criterion, input_size, num_classes, epochs, batch_size, test_runs):\n",
    "    \n",
    "    # Logistic regression model.\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(input_size, num_classes),\n",
    "        torch.nn.LogSoftmax(dim=1) \n",
    "    )\n",
    "    \n",
    "    # add all optimizers to a list\n",
    "    optimizer_list=[]\n",
    "    optimizer_list.append(optim.SGD(model.parameters(), lr=0.01))\n",
    "    optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9))\n",
    "    optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True))\n",
    "    optimizer_list.append(optim.Adagrad(model.parameters(), lr=0.01))\n",
    "    optimizer_list.append(optim.RMSprop(model.parameters(), lr=0.01))\n",
    "    optimizer_list.append(optim.Adam(model.parameters(), lr=0.01))\n",
    "    \n",
    "    test_losses = [] # store test losses for each optimizer\n",
    "    \n",
    "    # carry out training and testing for each optimizer and save the test losses for the number of test runs:\n",
    "    \n",
    "    for i in range(test_runs):\n",
    "        \n",
    "        # TRAIN\n",
    "        for optimizer in optimizer_list:\n",
    "            \n",
    "            # Logistic regression model.\n",
    "            model = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.Linear(input_size, num_classes),\n",
    "                torch.nn.LogSoftmax(dim=1) \n",
    "            )\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "    #             print('Epoch {}'.format(epoch+1))\n",
    "                for i, (images, labels) in enumerate(train_loader):\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward + backward + optimize\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Log the loss\n",
    "    #                 log_interval = 100\n",
    "    #                 if i % log_interval == 0:\n",
    "    #                     print('Current loss: {}'.format(loss))\n",
    "\n",
    "\n",
    "            # TEST\n",
    "            model.eval()\n",
    "            test_acc = 0\n",
    "            total_data = 0\n",
    "            loss = 0\n",
    "            with torch.no_grad():\n",
    "                for _, (images, labels) in enumerate(test_loader):\n",
    "                    output = model(images)\n",
    "                    pred = output.argmax(dim=1, keepdim=True)\n",
    "                    test_acc += pred.eq(labels.view_as(pred)).sum().item()\n",
    "                    total_data += len(images)\n",
    "                    loss = criterion(output, labels)\n",
    "\n",
    "#             print('Loss: {}'.format(loss))\n",
    "\n",
    "            test_acc /= total_data\n",
    "#             print('Test accuracy over {} data points: {}%'.format(total_data, test_acc * 100))\n",
    "            \n",
    "        test_losses.append(test_run_loss)\n",
    "#         print(test_losses)\n",
    "\n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return a list of the test losses at the end of each epoch.\n",
    "\"\"\"\n",
    "\n",
    "def train_test_trajectory(model, optimizer, train_loader, test_loader, criterion, epochs):\n",
    "\n",
    "    test_trajectory = []\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        \n",
    "        # TRAIN\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        # TEST\n",
    "        model.eval()\n",
    "        test_acc = 0\n",
    "        total_data = 0\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (images, labels) in enumerate(test_loader):\n",
    "                output = model(images)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                test_acc += pred.eq(labels.view_as(pred)).sum().item()\n",
    "                total_data += len(images)\n",
    "                loss = criterion(output, labels)\n",
    "#         print(loss.item())\n",
    "        test_trajectory.append(loss.item())\n",
    "\n",
    "    return test_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For a dataset, will loop through all the optimizers and save the test loss trajectories after training + testing.\n",
    "\"\"\"\n",
    "\n",
    "def trajectory_loss(train_loader, test_loader, criterion, input_size, num_classes, epochs, batch_size, test_runs):\n",
    "    \n",
    "    # Logistic regression model.\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(input_size, num_classes),\n",
    "        torch.nn.LogSoftmax(dim=1) \n",
    "    )\n",
    "    \n",
    "    test_losses = [] # store test losses for each optimizer\n",
    "    \n",
    "    # carry out training and testing for each optimizer and save the test losses for the number of test runs:\n",
    "    \n",
    "    for i in range(test_runs):\n",
    "        test_run_loss = []\n",
    "        \n",
    "        # SGD       \n",
    "        model = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.Linear(input_size, num_classes),\n",
    "                torch.nn.LogSoftmax(dim=1) \n",
    "        )\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "        trajectory = train_test_trajectory(model, optimizer, train_loader, test_loader, criterion, epochs)\n",
    "        for i in trajectory:\n",
    "            test_run_loss.append(i)\n",
    "        \n",
    "        # Momentum        \n",
    "        model = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.Linear(input_size, num_classes),\n",
    "                torch.nn.LogSoftmax(dim=1) \n",
    "        )\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "        trajectory = train_test_trajectory(model, optimizer, train_loader, test_loader, criterion, epochs)\n",
    "        for i in trajectory:\n",
    "            test_run_loss.append(i)\n",
    "        \n",
    "        # Adadelta        \n",
    "        model = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.Linear(input_size, num_classes),\n",
    "                torch.nn.LogSoftmax(dim=1) \n",
    "        )\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "        trajectory = train_test_trajectory(model, optimizer, train_loader, test_loader, criterion, epochs)\n",
    "        for i in trajectory:\n",
    "            test_run_loss.append(i)\n",
    "        \n",
    "        # Adagrad        \n",
    "        model = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.Linear(input_size, num_classes),\n",
    "                torch.nn.LogSoftmax(dim=1) \n",
    "        )\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "        trajectory = train_test_trajectory(model, optimizer, train_loader, test_loader, criterion, epochs)\n",
    "        for i in trajectory:\n",
    "            test_run_loss.append(i)\n",
    "        \n",
    "        # RMSprop       \n",
    "        model = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.Linear(input_size, num_classes),\n",
    "                torch.nn.LogSoftmax(dim=1) \n",
    "        )\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "        trajectory = train_test_trajectory(model, optimizer, train_loader, test_loader, criterion, epochs)\n",
    "        for i in trajectory:\n",
    "            test_run_loss.append(i)\n",
    "        \n",
    "        # Adam\n",
    "        model = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.Linear(input_size, num_classes),\n",
    "                torch.nn.LogSoftmax(dim=1) \n",
    "        )\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        trajectory = train_test_trajectory(model, optimizer, train_loader, test_loader, criterion, epochs)\n",
    "        for i in trajectory:\n",
    "            test_run_loss.append(i)\n",
    "   \n",
    "            \n",
    "        test_losses.append(test_run_loss)         \n",
    "\n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the average loss for each optimizer over several test runs.\n",
    "\"\"\"\n",
    "def calc_task_avg_loss(loss_list):\n",
    "    avg_loss = len(loss_list[0])*[0]\n",
    "    for test_run in range(len(loss_list)):\n",
    "        for optimizer in range(len(loss_list[test_run])):\n",
    "            avg_loss[optimizer] += loss_list[test_run][optimizer]\n",
    "\n",
    "    for i in range(len(avg_loss)):\n",
    "        avg_loss[i] /= len(loss_list) \n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLL since we include softmax as part of model.  \n",
    "criterion = nn.NLLLoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_logistic_regression_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "\n",
    "# Normalizer\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "input_size = 32*32*3\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_trajectory = trajectory_loss(train_loader, test_loader, criterion, input_size, num_classes, epochs, batch_size, test_runs)\n",
    "for i in loss_trajectory:\n",
    "    all_trajectories.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.385009765625, 1.5343563556671143, 9.765871047973633, 16.975919723510742, 3.4305944442749023, 2.4465889930725098, 1.5743026733398438, 1.7565749883651733, 18.759689331054688, 5.984659194946289, 10.059686660766602, 11.810853958129883], [2.2807648181915283, 2.4239768981933594, 27.430213928222656, 26.346155166625977, 1.2262992858886719, 3.0819363594055176, 1.8389556407928467, 1.748749852180481, 10.621614456176758, 19.63835334777832, 6.363015651702881, 10.001669883728027], [1.9121567010879517, 2.067091464996338, 17.15863609313965, 3.398571491241455, 2.518282890319824, 4.72898006439209, 1.3821437358856201, 1.544044017791748, 12.258016586303711, 10.70290470123291, 10.811775207519531, 8.383513450622559]]\n"
     ]
    }
   ],
   "source": [
    "print(all_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">SGD</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Momentum</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Nesterov</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Adagrad</th>\n",
       "      <th colspan=\"2\" halign=\"left\">RMSProp</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Adam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cifar10_logistic_regression</th>\n",
       "      <th>0</th>\n",
       "      <td>1.385010</td>\n",
       "      <td>1.534356</td>\n",
       "      <td>9.765871</td>\n",
       "      <td>16.975920</td>\n",
       "      <td>3.430594</td>\n",
       "      <td>2.446589</td>\n",
       "      <td>1.574303</td>\n",
       "      <td>1.756575</td>\n",
       "      <td>18.759689</td>\n",
       "      <td>5.984659</td>\n",
       "      <td>10.059687</td>\n",
       "      <td>11.810854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.280765</td>\n",
       "      <td>2.423977</td>\n",
       "      <td>27.430214</td>\n",
       "      <td>26.346155</td>\n",
       "      <td>1.226299</td>\n",
       "      <td>3.081936</td>\n",
       "      <td>1.838956</td>\n",
       "      <td>1.748750</td>\n",
       "      <td>10.621614</td>\n",
       "      <td>19.638353</td>\n",
       "      <td>6.363016</td>\n",
       "      <td>10.001670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.912157</td>\n",
       "      <td>2.067091</td>\n",
       "      <td>17.158636</td>\n",
       "      <td>3.398571</td>\n",
       "      <td>2.518283</td>\n",
       "      <td>4.728980</td>\n",
       "      <td>1.382144</td>\n",
       "      <td>1.544044</td>\n",
       "      <td>12.258017</td>\n",
       "      <td>10.702905</td>\n",
       "      <td>10.811775</td>\n",
       "      <td>8.383513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    SGD             Momentum             \\\n",
       "                                      0         1          0          1   \n",
       "cifar10_logistic_regression 0  1.385010  1.534356   9.765871  16.975920   \n",
       "                            1  2.280765  2.423977  27.430214  26.346155   \n",
       "                            2  1.912157  2.067091  17.158636   3.398571   \n",
       "\n",
       "                               Nesterov             Adagrad            \\\n",
       "                                      0         1         0         1   \n",
       "cifar10_logistic_regression 0  3.430594  2.446589  1.574303  1.756575   \n",
       "                            1  1.226299  3.081936  1.838956  1.748750   \n",
       "                            2  2.518283  4.728980  1.382144  1.544044   \n",
       "\n",
       "                                 RMSProp                  Adam             \n",
       "                                       0          1          0          1  \n",
       "cifar10_logistic_regression 0  18.759689   5.984659  10.059687  11.810854  \n",
       "                            1  10.621614  19.638353   6.363016  10.001670  \n",
       "                            2  12.258017  10.702905  10.811775   8.383513  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=all_trajectories)\n",
    "\n",
    "tasks = ['cifar10_logistic_regression']\n",
    "runs = range(test_runs)\n",
    "df.index = pd.MultiIndex.from_product([tasks, runs])\n",
    "\n",
    "optimizers = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "epoch_ind = range(epochs)\n",
    "df.columns = pd.MultiIndex.from_product([optimizers, epoch_ind])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_results = train_test(train_loader, test_loader, criterion, input_size, num_classes, num_epochs, batch_size, test_runs)\n",
    "all_logistic_regression_loss.append(calc_task_avg_loss(loss_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-100 dataset\n",
    "\n",
    "# Normalizer\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "input_size = 32*32*3\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_results = train_test(train_loader, test_loader, criterion, input_size, num_classes, num_epochs, batch_size, 3)\n",
    "all_logistic_regression_loss.append(calc_task_avg_loss(loss_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-MNIST dataset\n",
    "\n",
    "# Normalizer\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "input_size = 28*28\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_results = train_test(train_loader, test_loader, criterion, input_size, num_classes, num_epochs, batch_size, 3)\n",
    "all_logistic_regression_loss.append(calc_task_avg_loss(loss_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "input_size = 28 * 28    # 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_results = train_test(train_loader, test_loader, criterion, input_size, num_classes, num_epochs, batch_size, 3)\n",
    "all_logistic_regression_loss.append(calc_task_avg_loss(loss_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVHN dataset\n",
    "\n",
    "# Normalizer\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.SVHN(root='./data', split=\"train\",\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.SVHN(root='./data', split=\"test\",\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "input_size = 32*32*3\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_results = train_test(train_loader, test_loader, criterion, input_size, num_classes, num_epochs, batch_size, 3)\n",
    "all_loss.append(loss_results)\n",
    "# all_logistic_regression_loss.append(calc_task_avg_loss(loss_results)) # get avg loss for all test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save avg test run results to csv\n",
    "index = ['logistic_regression_cifar100','logistic_regression_cifar10','logistic_regression_fmnist','logistic_regression_mnist','logistic_regression_svhn']\n",
    "col = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "df = pd.DataFrame(data=all_logistic_regression_loss, index=index, columns=col)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_avg_losses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_losses = np.asarray(all_logistic_regression_loss)\n",
    "# test_losses\n",
    "# normalized_test_losses = []\n",
    "\n",
    "# for i in range(len(test_losses)):\n",
    "#     mean = np.mean(test_losses[i])\n",
    "#     minus_mean = test_losses[i] - mean\n",
    "#     normalized_test_losses.append((minus_mean)/np.linalg.norm(minus_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = ['logistic_regression_cifar100','logistic_regression_cifar10','logistic_regression_fmnist','logistic_regression_mnist','logistic_regression_svhn']\n",
    "# col = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "# df = pd.DataFrame(data=normalized_test_losses, index=index, columns=col)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('all_logistic_regression_avg_normalized_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
