{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i think this combines everything that i did below so i'll remove/clean up the extra stuff above once i confirm this function works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_trajectory(optimizer, model, x_dataset, y_dataset, x_test, y_test, criterion, epochs):\n",
    "    # Main optimization loop\n",
    "    test_trajectory = []\n",
    "    for t in range(epochs):\n",
    "        # Set the gradients to 0.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the current predicted labels from x_dataset\n",
    "        y_predicted = model(x_dataset)\n",
    "        \n",
    "        # See how far off the prediction is\n",
    "        current_loss = criterion(y_predicted, y_dataset)\n",
    "\n",
    "        # Compute the gradient of the loss with respect to A and b\n",
    "        current_loss.backward()\n",
    "        \n",
    "        # Update A and b accordingly\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(f\"train loss = {current_loss}\")\n",
    "        \n",
    "        y_predicted = model(x_test)\n",
    "    \n",
    "        loss = criterion(y_predicted, y_test)\n",
    "\n",
    "        # Get index with highest probability.\n",
    "        predicted_labels = torch.argmax(y_predicted, dim=1)\n",
    "\n",
    "        correct = (predicted_labels == y_test).sum()\n",
    "        \n",
    "#         print('test loss: {}'.format(loss.item()))\n",
    "        test_trajectory.append(loss.item())\n",
    "        \n",
    "    return test_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_losses(samples, dim, optimizer_list, criterion, test_runs):\n",
    "    loss_list = []\n",
    "    \n",
    "    samples = samples #number of samples from each distribution\n",
    "    dim = dim\n",
    "    output_dim = 2\n",
    "    \n",
    "    # means of the distributions\n",
    "    mean1 = 0\n",
    "    mean2 = 3/math.sqrt(dim)\n",
    "    \n",
    "    for i in range(test_runs):\n",
    "        test_run_loss = []\n",
    "        for optimizer in optimizer_list:\n",
    "            \n",
    "            # Logistic regression model\n",
    "            model = torch.nn.Sequential(\n",
    "                torch.nn.Linear(dim, samples),\n",
    "                torch.nn.LogSoftmax(dim=1) \n",
    "            )\n",
    "\n",
    "            # get training samples\n",
    "            x_dataset = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                                np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "            # get training labels\n",
    "            gaussian1_labels = [1]*int(samples)\n",
    "            gaussian2_labels = [0]*int(samples)\n",
    "            y_dataset = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor\n",
    "\n",
    "            # get testing samples\n",
    "            x_test = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                             np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "            # get testing labels \n",
    "            gaussian1_labels = [1]*int(samples)\n",
    "            gaussian2_labels = [0]*int(samples)\n",
    "            y_test = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor\n",
    "\n",
    "            train(samples, optimizer, model, x_dataset, y_dataset, criterion)\n",
    "            test_run_loss.append(test(model, x_test, y_test, criterion))\n",
    "            \n",
    "        loss_list.append(test_run_loss)\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory_losses(samples, dim, test_runs, epochs):\n",
    "    loss_list = []\n",
    "    \n",
    "    samples = samples #number of samples from each distribution\n",
    "    dim = dim\n",
    "    output_dim = 2\n",
    "\n",
    "    # means of the distributions\n",
    "    mean1 = 0\n",
    "    mean2 = 3/math.sqrt(dim)\n",
    "    \n",
    "    # Logistic regression model\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(dim, samples),\n",
    "        torch.nn.LogSoftmax(dim=1) \n",
    "    )\n",
    "    \n",
    "    criterion = nn.NLLLoss() \n",
    "    optimizer_list=[]\n",
    "    optimizer_list.append(optim.SGD(model.parameters(), lr=0.01))\n",
    "    optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9))\n",
    "    optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True))\n",
    "    optimizer_list.append(optim.Adagrad(model.parameters(), lr=0.01))\n",
    "    optimizer_list.append(optim.RMSprop(model.parameters(), lr=0.01))\n",
    "    optimizer_list.append(optim.Adam(model.parameters(), lr=0.01))\n",
    "    \n",
    "    for i in range(test_runs):\n",
    "        test_run_loss = []\n",
    "        \n",
    "        for optimizer in optimizer_list:\n",
    "\n",
    "            # get training samples\n",
    "            x_dataset = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                                np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "            # get training labels\n",
    "            gaussian1_labels = [1]*int(samples)\n",
    "            gaussian2_labels = [0]*int(samples)\n",
    "            y_dataset = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor\n",
    "\n",
    "            # get testing samples\n",
    "            x_test = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                             np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "            # get testing labels \n",
    "            gaussian1_labels = [1]*int(samples)\n",
    "            gaussian2_labels = [0]*int(samples)\n",
    "            y_test = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor\n",
    "            \n",
    "            trajectory = train_test_trajectory(optimizer, model, x_dataset, y_dataset, x_test, y_test, criterion, epochs)      \n",
    "            for l in trajectory:\n",
    "                test_run_loss.append(l)\n",
    "           \n",
    "        loss_list.append(test_run_loss)\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the average loss for each optimizer over several test runs.\n",
    "\"\"\"\n",
    "def calc_task_avg_loss(loss_list):\n",
    "    avg_loss = len(loss_list[0])*[0]\n",
    "    for test_run in range(len(loss_list)):\n",
    "        for optimizer in range(len(loss_list[test_run])):\n",
    "            avg_loss[optimizer] += loss_list[test_run][optimizer]\n",
    "\n",
    "    for i in range(len(avg_loss)):\n",
    "        avg_loss[i] /= len(loss_list) \n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "test_runs = 10\n",
    "output_dim = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exactly parameterized\n",
    "samples = 20 #number of samples from each distribution\n",
    "dim = 20\n",
    "\n",
    "exact_param = get_trajectory_losses(samples, dim, test_runs, epochs)\n",
    "\n",
    "for i in exact_param:\n",
    "    all_losses.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overparameterized\n",
    "samples = 10 #number of samples from each distribution\n",
    "dim = 300\n",
    "\n",
    "over_param = get_trajectory_losses(samples, dim, test_runs, epochs)\n",
    "for i in over_param:\n",
    "    all_losses.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# underparameterized\n",
    "samples = 50 #number of samples from each distribution\n",
    "dim = 3\n",
    "\n",
    "under_param = get_trajectory_losses(samples, dim, test_runs, epochs)\n",
    "for i in under_param:\n",
    "    all_losses.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=all_losses)\n",
    "\n",
    "tasks = ['logistic_regression_gaussian_exact_param','logistic_regression_gaussian_over_param','logistic_regression_gaussian_under_param']\n",
    "runs = range(test_runs)\n",
    "df.index = pd.MultiIndex.from_product([tasks, runs])\n",
    "\n",
    "optimizers = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "epoch_ind = range(epochs)\n",
    "df.columns = pd.MultiIndex.from_product([optimizers, epoch_ind])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('logistic_regression_gaussian_trajectory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['logistic_regression_gaussian_exact_param','logistic_regression_gaussian_over_param','logistic_regression_gaussian_over_param']\n",
    "col = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "df = pd.DataFrame(data=all_losses, index=index, columns=col)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE STUFF BELOW! repetitive/testing stuff, will delete later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n, optimizer, model, x_dataset, y_dataset, criterion):\n",
    "    # Main optimization loop\n",
    "    for t in range(500):\n",
    "        # Set the gradients to 0.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the current predicted labels from x_dataset\n",
    "        y_predicted = model(x_dataset)\n",
    "        \n",
    "        # See how far off the prediction is\n",
    "        current_loss = criterion(y_predicted, y_dataset)\n",
    "\n",
    "        # Compute the gradient of the loss with respect to A and b\n",
    "        current_loss.backward()\n",
    "        \n",
    "        # Update A and b accordingly\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"loss = {current_loss}\")\n",
    "        \n",
    "        y_predicted = model(x_test)\n",
    "    \n",
    "        loss = criterion(y_predicted, y_test)\n",
    "\n",
    "        # Get index with highest probability.\n",
    "        predicted_labels = torch.argmax(y_predicted, dim=1)\n",
    "\n",
    "        correct = (predicted_labels == y_test).sum()\n",
    "        \n",
    "        print('Loss: {}'.format(loss.item()))\n",
    "    \n",
    "    return current_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x_test, y_test, loss_fn):\n",
    "    # Returns accuracy, loss.\n",
    "    \n",
    "    # Get predicted probability vectors from test data.\n",
    "    y_predicted = model(x_test)\n",
    "    \n",
    "    loss = loss_fn(y_predicted, y_test)\n",
    "    \n",
    "    # Get index with highest probability.\n",
    "    predicted_labels = torch.argmax(y_predicted, dim=1)\n",
    "    \n",
    "    correct = (predicted_labels == y_test).sum()\n",
    "    \n",
    "#     print('Accuracy: {}'.format(correct.item()/len(y_test)))\n",
    "#     print('Loss: {}'.format(loss.item()))\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, x_dataset, y_dataset):\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     ind = 0\n",
    "#     loss = 0\n",
    "#     for sample in x_dataset:\n",
    "#         sample = sample.unsqueeze(dim=0) #add an extra dimension to sample point bc pytorch syntax; e.g. [0,0] -> [[0,0]]\n",
    "        \n",
    "#         output = model(sample) \n",
    "#         print(\"OUTPUT\")\n",
    "#         print(output)\n",
    "#         print()\n",
    "        \n",
    "#         _, predicted = torch.max(output.data, 1) #use _ to discard first output; get class with highest probability\n",
    "#         print(\"PREDICTED CLASS\")\n",
    "#         print(predicted)\n",
    "#         print()\n",
    "        \n",
    "#         print(\"ACTUAL CLASS\")\n",
    "#         print(y_dataset[ind]) # actual class of sample point\n",
    "        \n",
    "#         # count number of correct classifications\n",
    "#         print(predicted)\n",
    "#         if predicted == y_dataset[ind]:\n",
    "#             correct += 1\n",
    "#             print(\"CORRECT******\")\n",
    "        \n",
    "#         # trying to set labels correctly and find loss here; getting value error \n",
    "#         label = 0\n",
    "#         if y_dataset[ind] == 1:\n",
    "#             label = torch.tensor([1,0])\n",
    "#         else: \n",
    "#             label = torch.tensor([0,1])\n",
    "#         print(\"LABEL \", label)\n",
    "#         print(\"OUTPUT \", output)\n",
    "#         loss = criterion(output, label)\n",
    "        \n",
    "#         print(loss)    \n",
    "    \n",
    "#     total += y_dataset.size(0) # Total number of labels\n",
    "#     acc = correct.item()/total\n",
    "#     print('Loss: {}'.format(loss))\n",
    "#     print('Test accuracy over {} data points: {}%'.format(total_data, test_acc * 100))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exactly parameterized\n",
    "Number of parameters = number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = 20 #number of samples from each distribution\n",
    "dim = 20\n",
    "output_dim = 2\n",
    "\n",
    "# means of the distributions\n",
    "mean1 = 0\n",
    "mean2 = 3/math.sqrt(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get training samples\n",
    "\n",
    "# sample from 2 gaussians\n",
    "x_dataset = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                    np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "# label = 1 for first dist., label = 0 for second dist.\n",
    "gaussian1_labels = [1]*int(samples)\n",
    "gaussian2_labels = [0]*int(samples)\n",
    "y_dataset = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor\n",
    "\n",
    "# print(y_dataset)\n",
    "# print(x_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get testing samples\n",
    "\n",
    "x_test = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                 np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "# label = 1 for first dist., label = 0 for second dist.\n",
    "gaussian1_labels = [1]*int(samples)\n",
    "gaussian2_labels = [0]*int(samples)\n",
    "y_test = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim, output_dim),\n",
    "    torch.nn.LogSoftmax(dim=1) \n",
    ")\n",
    "\n",
    "# Use NLL since we include softmax as part of model \n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store loss values of exactly parameterized test runs\n",
    "exact_param = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_trajectory(optimizer, model, x_dataset, y_dataset, x_test, y_test, criterion, epochs):\n",
    "    \n",
    "    # Main optimization loop\n",
    "    test_trajectory = []\n",
    "    for t in range(20):\n",
    "        # Set the gradients to 0.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the current predicted labels from x_dataset\n",
    "        y_predicted = model(x_dataset)\n",
    "        \n",
    "        # See how far off the prediction is\n",
    "        current_loss = criterion(y_predicted, y_dataset)\n",
    "\n",
    "        # Compute the gradient of the loss with respect to A and b\n",
    "        current_loss.backward()\n",
    "         \n",
    "        # Update A and b accordingly\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"loss = {current_loss}\")\n",
    "        \n",
    "        y_predicted = model(x_test)\n",
    "    \n",
    "        loss = criterion(y_predicted, y_test)\n",
    "\n",
    "        # Get index with highest probability.\n",
    "        predicted_labels = torch.argmax(y_predicted, dim=1)\n",
    "\n",
    "        correct = (predicted_labels == y_test).sum()\n",
    "        \n",
    "        print('Loss: {}'.format(loss.item()))\n",
    "        test_trajectory.append(loss.item())\n",
    "        \n",
    "    return test_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train_test_trajectory(optimizer, model, x_dataset, y_dataset, x_test, y_test, criterion, epochs)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "exact_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "exact_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "exact_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "exact_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "exact_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "exact_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store losses in overall loss list\n",
    "test_losses.append(exact_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overparameterized\n",
    "Number of parameters >> number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = 50 #number of samples from each distribution\n",
    "dim = 200\n",
    "output_dim = 2\n",
    "\n",
    "# means of the distributions\n",
    "mean1 = 0\n",
    "mean2 = 3/math.sqrt(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get training samples\n",
    "\n",
    "# sample from 2 gaussians\n",
    "x_dataset = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                    np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "# label = 1 for first dist., label = 0 for second dist.\n",
    "gaussian1_labels = [1]*int(samples)\n",
    "gaussian2_labels = [0]*int(samples)\n",
    "y_dataset = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor\n",
    "\n",
    "# print(y_dataset)\n",
    "# print(x_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get testing samples\n",
    "\n",
    "x_test = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                 np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "# label = 1 for first dist., label = 0 for second dist.\n",
    "gaussian1_labels = [1]*int(samples)\n",
    "gaussian2_labels = [0]*int(samples)\n",
    "y_test = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim, samples),\n",
    "    torch.nn.LogSoftmax(dim=1) \n",
    ")\n",
    "\n",
    "# Use NLL since we include softmax as part of model \n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_param = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "over_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "over_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "over_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "over_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "over_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "over_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store losses in overall loss list\n",
    "test_losses.append(over_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underparameterized\n",
    "Number of parameters << number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = 50 #number of samples from each distribution\n",
    "dim = 3\n",
    "output_dim = 2\n",
    "\n",
    "# means of the distributions\n",
    "mean1 = 0\n",
    "mean2 = 3/math.sqrt(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get training samples\n",
    "\n",
    "# sample from 2 gaussians\n",
    "x_dataset = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                    np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "# label = 1 for first dist., label = 0 for second dist.\n",
    "gaussian1_labels = [1]*int(samples)\n",
    "gaussian2_labels = [0]*int(samples)\n",
    "y_dataset = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor\n",
    "\n",
    "# print(y_dataset)\n",
    "# print(x_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get testing samples\n",
    "\n",
    "x_test = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                 np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "# label = 1 for first dist., label = 0 for second dist.\n",
    "gaussian1_labels = [1]*int(samples)\n",
    "gaussian2_labels = [0]*int(samples)\n",
    "y_test = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim, samples),\n",
    "    torch.nn.LogSoftmax(dim=1) \n",
    ")\n",
    "\n",
    "# Use NLL since we include softmax as part of model \n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store underparameterized test run losses\n",
    "under_param = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "under_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "under_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "under_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "under_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "under_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train(samples, optimizer, model, x_dataset, y_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test(model, x_test, y_test, criterion)\n",
    "under_param.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store losses in overall loss list\n",
    "test_losses.append(under_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['logistic_regression_gaussian_exact_param','logistic_regression_gaussian_over_param','logistic_regression_gaussian_over_param']\n",
    "col = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "df = pd.DataFrame(data=test_losses, index=index, columns=col)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('logistic_regression_gaussian_loss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = np.asarray(test_losses)\n",
    "test_losses\n",
    "normalized_test_losses = []\n",
    "\n",
    "for i in range(len(test_losses)):\n",
    "    mean = np.mean(test_losses[i])\n",
    "    minus_mean = test_losses[i] - mean\n",
    "    normalized_test_losses.append((minus_mean)/np.linalg.norm(minus_mean))\n",
    "print(normalized_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['logistic_regression_gaussian_exact_param','logistic_regression_gaussian_over_param','logistic_regression_gaussian_over_param']\n",
    "col = ['SGD','Momentum','Nesterov','Adagrad','RMSProp','Adam']\n",
    "df = pd.DataFrame(data=normalized_test_losses, index=index, columns=col)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('logistic_regression_gaussian_normalized_loss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overparameterized dimensions vs. losses graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_losses(samples, dim, optimizer, criterion):\n",
    "    samples = samples #number of samples from each distribution\n",
    "    dim = dim\n",
    "    output_dim = 2\n",
    "\n",
    "    # means of the distributions\n",
    "    mean1 = 0\n",
    "    mean2 = 3/math.sqrt(dim)\n",
    "\n",
    "    # get training samples\n",
    "    x_dataset = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                        np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "    # get training labels\n",
    "    gaussian1_labels = [1]*int(samples)\n",
    "    gaussian2_labels = [0]*int(samples)\n",
    "    y_dataset = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor\n",
    "\n",
    "    # get testing samples\n",
    "    x_test = torch.Tensor(np.vstack((np.random.normal(mean1, 1, size=(samples, dim)),\n",
    "                                     np.random.normal(mean2, 1, size=(samples, dim)))))\n",
    "\n",
    "    # get testing labels \n",
    "    gaussian1_labels = [1]*int(samples)\n",
    "    gaussian2_labels = [0]*int(samples)\n",
    "    y_test = torch.tensor(gaussian1_labels+gaussian2_labels,dtype=torch.long) # combine labels and convert to tensor\n",
    "    \n",
    "    # Logistic regression model\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(dim, samples),\n",
    "        torch.nn.LogSoftmax(dim=1) \n",
    "    )\n",
    "    \n",
    "    train(samples, optimizer, model, x_dataset, y_dataset, criterion)\n",
    "    return test(model, x_test, y_test, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer_list=[]\n",
    "optimizer_list.append(optim.SGD(model.parameters(), lr=0.01))\n",
    "optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9))\n",
    "optimizer_list.append(optim.SGD(model.parameters(), lr=0.01,momentum=0.9,nesterov=True))\n",
    "optimizer_list.append(optim.Adagrad(model.parameters(), lr=0.01))\n",
    "optimizer_list.append(optim.RMSprop(model.parameters(), lr=0.01))\n",
    "optimizer_list.append(optim.Adam(model.parameters(), lr=0.01))\n",
    "\n",
    "optimizer_names=['SGD', 'Momentum', 'Nesterov', 'Adagrad', 'RMSprop', 'Adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INDIVIDUAL GRAPHS\n",
    "\n",
    "samples = 100\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "for opt in range(len(optimizer_list)):\n",
    "    losses = []\n",
    "    sample_sizes = []\n",
    "    for dim in range(1,500,20):\n",
    "        losses.append(get_losses(samples, dim, optimizer_list[opt], criterion))\n",
    "        sample_sizes.append(dim)\n",
    "    plt.plot(sample_sizes, losses)\n",
    "    plt.title('Logistic regression losses for ' + optimizer_names[opt]+' with '+str(samples)+' samples')\n",
    "    plt.xlabel('Dimensions')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "#     plt.legend(['SGD', 'Momentum', 'Nesterov', 'Adagrad', 'RMSprop', 'Adam'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVERLAY GRAPH\n",
    "\n",
    "samples = 100\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "for optimizer in optimizer_list:\n",
    "    losses = []\n",
    "    sample_sizes = []\n",
    "    for dim in range(1,500,20):\n",
    "        losses.append(get_losses(samples, dim, optimizer, criterion))\n",
    "        sample_sizes.append(dim)\n",
    "    plt.plot(sample_sizes, losses)\n",
    "    plt.title('Logistic regression losses with '+str(samples)+' samples')\n",
    "    plt.xlabel('Dimensions')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['SGD', 'Momentum', 'Nesterov', 'Adagrad', 'RMSprop', 'Adam'], loc='upper left',bbox_to_anchor=(1, 0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
